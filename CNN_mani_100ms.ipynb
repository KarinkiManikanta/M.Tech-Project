{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(575983, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCC0</th>\n",
       "      <th>MFCC1</th>\n",
       "      <th>MFCC2</th>\n",
       "      <th>MFCC3</th>\n",
       "      <th>MFCC4</th>\n",
       "      <th>MFCC5</th>\n",
       "      <th>MFCC6</th>\n",
       "      <th>MFCC7</th>\n",
       "      <th>MFCC8</th>\n",
       "      <th>MFCC9</th>\n",
       "      <th>MFCC10</th>\n",
       "      <th>MFCC11</th>\n",
       "      <th>MFCC12</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.339339</td>\n",
       "      <td>-58.503360</td>\n",
       "      <td>28.969193</td>\n",
       "      <td>-16.135848</td>\n",
       "      <td>-0.850935</td>\n",
       "      <td>-7.166208</td>\n",
       "      <td>24.007454</td>\n",
       "      <td>-25.177117</td>\n",
       "      <td>7.820241</td>\n",
       "      <td>1.586773</td>\n",
       "      <td>-0.016230</td>\n",
       "      <td>-4.160415</td>\n",
       "      <td>-2.928838</td>\n",
       "      <td>crying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10.255503</td>\n",
       "      <td>-58.766382</td>\n",
       "      <td>28.839630</td>\n",
       "      <td>-15.665957</td>\n",
       "      <td>0.201420</td>\n",
       "      <td>-7.582272</td>\n",
       "      <td>24.395316</td>\n",
       "      <td>-24.310047</td>\n",
       "      <td>9.044284</td>\n",
       "      <td>1.180245</td>\n",
       "      <td>1.338979</td>\n",
       "      <td>-2.547351</td>\n",
       "      <td>-3.876356</td>\n",
       "      <td>crying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-10.219711</td>\n",
       "      <td>-47.300163</td>\n",
       "      <td>29.763146</td>\n",
       "      <td>-17.593168</td>\n",
       "      <td>3.772775</td>\n",
       "      <td>-9.047093</td>\n",
       "      <td>18.010719</td>\n",
       "      <td>-21.991444</td>\n",
       "      <td>12.146181</td>\n",
       "      <td>-2.855262</td>\n",
       "      <td>-0.514320</td>\n",
       "      <td>-0.502388</td>\n",
       "      <td>-3.394465</td>\n",
       "      <td>crying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10.335775</td>\n",
       "      <td>-36.359484</td>\n",
       "      <td>28.162880</td>\n",
       "      <td>-7.049695</td>\n",
       "      <td>27.120645</td>\n",
       "      <td>-3.100069</td>\n",
       "      <td>16.331104</td>\n",
       "      <td>-2.871143</td>\n",
       "      <td>28.954943</td>\n",
       "      <td>-1.509898</td>\n",
       "      <td>1.557353</td>\n",
       "      <td>5.172644</td>\n",
       "      <td>-2.621226</td>\n",
       "      <td>crying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-10.328217</td>\n",
       "      <td>-20.086843</td>\n",
       "      <td>13.868843</td>\n",
       "      <td>-5.345090</td>\n",
       "      <td>23.884885</td>\n",
       "      <td>-9.032826</td>\n",
       "      <td>10.503083</td>\n",
       "      <td>0.159714</td>\n",
       "      <td>21.487532</td>\n",
       "      <td>-3.494115</td>\n",
       "      <td>4.201391</td>\n",
       "      <td>3.049172</td>\n",
       "      <td>6.644954</td>\n",
       "      <td>crying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MFCC0      MFCC1      MFCC2      MFCC3      MFCC4     MFCC5      MFCC6  \\\n",
       "0 -10.339339 -58.503360  28.969193 -16.135848  -0.850935 -7.166208  24.007454   \n",
       "1 -10.255503 -58.766382  28.839630 -15.665957   0.201420 -7.582272  24.395316   \n",
       "2 -10.219711 -47.300163  29.763146 -17.593168   3.772775 -9.047093  18.010719   \n",
       "3 -10.335775 -36.359484  28.162880  -7.049695  27.120645 -3.100069  16.331104   \n",
       "4 -10.328217 -20.086843  13.868843  -5.345090  23.884885 -9.032826  10.503083   \n",
       "\n",
       "       MFCC7      MFCC8     MFCC9    MFCC10    MFCC11    MFCC12   Label  \n",
       "0 -25.177117   7.820241  1.586773 -0.016230 -4.160415 -2.928838  crying  \n",
       "1 -24.310047   9.044284  1.180245  1.338979 -2.547351 -3.876356  crying  \n",
       "2 -21.991444  12.146181 -2.855262 -0.514320 -0.502388 -3.394465  crying  \n",
       "3  -2.871143  28.954943 -1.509898  1.557353  5.172644 -2.621226  crying  \n",
       "4   0.159714  21.487532 -3.494115  4.201391  3.049172  6.644954  crying  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read MFCCs feature CSV file of audio of 100ms block\n",
    "\n",
    "path=\"./home/bsplab/Desktop/manikanta/CNN_mani/Traing_100ms/CRYING/\"\n",
    "#filename=os.path.join(path,\"100ms_7C.csv\")\n",
    "traindata =pd.read_csv(\"/home/bsplab/Desktop/manikanta/CNN_mani/Traing_100ms/CRYING/individual_100ms.csv\",na_values=['NA','?'])\n",
    "traindata .columns=['MFCC0', 'MFCC1','MFCC2','MFCC3','MFCC4','MFCC5','MFCC6','MFCC7','MFCC8', \n",
    "            'MFCC9', 'MFCC10' ,'MFCC11', 'MFCC12', 'Label']\n",
    "#df=pd.read_csv(filename,na_values=['NA','?'])\n",
    "filename_write = os.path.join(path,\"individual_100ms_cnn.csv.csv\")\n",
    "print(traindata .shape)\n",
    "traindata .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575983, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata=traindata.dropna()\n",
    "traindata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCC0</th>\n",
       "      <th>MFCC1</th>\n",
       "      <th>MFCC2</th>\n",
       "      <th>MFCC3</th>\n",
       "      <th>MFCC4</th>\n",
       "      <th>MFCC5</th>\n",
       "      <th>MFCC6</th>\n",
       "      <th>MFCC7</th>\n",
       "      <th>MFCC8</th>\n",
       "      <th>MFCC9</th>\n",
       "      <th>MFCC10</th>\n",
       "      <th>MFCC11</th>\n",
       "      <th>MFCC12</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.604626</td>\n",
       "      <td>-8.172296</td>\n",
       "      <td>5.206748</td>\n",
       "      <td>-2.383627</td>\n",
       "      <td>-21.005279</td>\n",
       "      <td>16.550854</td>\n",
       "      <td>-22.240188</td>\n",
       "      <td>-2.184232</td>\n",
       "      <td>17.962110</td>\n",
       "      <td>-38.702665</td>\n",
       "      <td>30.683808</td>\n",
       "      <td>-23.026454</td>\n",
       "      <td>4.921159</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.771606</td>\n",
       "      <td>13.864471</td>\n",
       "      <td>-32.058541</td>\n",
       "      <td>10.250745</td>\n",
       "      <td>-21.167710</td>\n",
       "      <td>8.608871</td>\n",
       "      <td>-22.244622</td>\n",
       "      <td>-18.606278</td>\n",
       "      <td>2.309756</td>\n",
       "      <td>-15.449925</td>\n",
       "      <td>3.155800</td>\n",
       "      <td>-9.158134</td>\n",
       "      <td>3.111149</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.758959</td>\n",
       "      <td>-6.394277</td>\n",
       "      <td>2.617576</td>\n",
       "      <td>1.732703</td>\n",
       "      <td>-27.661945</td>\n",
       "      <td>29.846086</td>\n",
       "      <td>-30.306968</td>\n",
       "      <td>0.760964</td>\n",
       "      <td>20.359883</td>\n",
       "      <td>-40.511891</td>\n",
       "      <td>41.736541</td>\n",
       "      <td>-25.518602</td>\n",
       "      <td>14.693751</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.917437</td>\n",
       "      <td>14.327343</td>\n",
       "      <td>-16.532813</td>\n",
       "      <td>0.810541</td>\n",
       "      <td>-12.540902</td>\n",
       "      <td>9.442987</td>\n",
       "      <td>-25.251732</td>\n",
       "      <td>-29.238576</td>\n",
       "      <td>10.375724</td>\n",
       "      <td>-26.777831</td>\n",
       "      <td>4.799518</td>\n",
       "      <td>-8.528685</td>\n",
       "      <td>-3.050550</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.644729</td>\n",
       "      <td>-37.392616</td>\n",
       "      <td>-1.961913</td>\n",
       "      <td>48.395111</td>\n",
       "      <td>-30.597397</td>\n",
       "      <td>34.214259</td>\n",
       "      <td>8.289310</td>\n",
       "      <td>-25.629234</td>\n",
       "      <td>-2.689849</td>\n",
       "      <td>-10.126319</td>\n",
       "      <td>-8.749178</td>\n",
       "      <td>4.808536</td>\n",
       "      <td>-10.116423</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MFCC0      MFCC1      MFCC2      MFCC3      MFCC4      MFCC5      MFCC6  \\\n",
       "0 -5.604626  -8.172296   5.206748  -2.383627 -21.005279  16.550854 -22.240188   \n",
       "1 -1.771606  13.864471 -32.058541  10.250745 -21.167710   8.608871 -22.244622   \n",
       "2 -2.758959  -6.394277   2.617576   1.732703 -27.661945  29.846086 -30.306968   \n",
       "3 -3.917437  14.327343 -16.532813   0.810541 -12.540902   9.442987 -25.251732   \n",
       "4 -1.644729 -37.392616  -1.961913  48.395111 -30.597397  34.214259   8.289310   \n",
       "\n",
       "       MFCC7      MFCC8      MFCC9     MFCC10     MFCC11     MFCC12   Label  \n",
       "0  -2.184232  17.962110 -38.702665  30.683808 -23.026454   4.921159  speech  \n",
       "1 -18.606278   2.309756 -15.449925   3.155800  -9.158134   3.111149   music  \n",
       "2   0.760964  20.359883 -40.511891  41.736541 -25.518602  14.693751  speech  \n",
       "3 -29.238576  10.375724 -26.777831   4.799518  -8.528685  -3.050550   music  \n",
       "4 -25.629234  -2.689849 -10.126319  -8.749178   4.808536 -10.116423  speech  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle\n",
    "np.random.seed(42)# set a seed so that the results are consistent\n",
    "traindata = traindata.reindex(np.random.permutation(traindata.index))\n",
    "traindata.reset_index(inplace=True, drop=True)\n",
    "traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling is:['ac' 'crying' 'music' 'speech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsplab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# Encode to a 2D matrix for training\n",
    "Label = encode_text_index(traindata,'Label')\n",
    "print(\"Labelling is:{}\".format(Label))\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Sperating Independent variable and Target Variable\n",
    "x,y=to_xy(traindata,'Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = traindata.iloc[:,0:13]\n",
    "Y = traindata.iloc[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.array(X_train)\n",
    "testT = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1 = np.array(y_train)\n",
    "y_test1 = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "#import keras.utils\n",
    "from keras.utils import to_categorical\n",
    "from keras import utils as np_utils\n",
    "y_train= to_categorical(y_train1)\n",
    "y_test= to_categorical(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(431987, 13, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143996, 13, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsplab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(13, 1), padding=\"same\")`\n",
      "  \n",
      "/home/bsplab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(13,1)))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(4, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsplab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "431987/431987 [==============================] - 10s 24us/step - loss: 0.1448 - acc: 0.9515\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.14483, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-01.hdf5\n",
      "Epoch 2/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.1028 - acc: 0.9648\n",
      "\n",
      "Epoch 00002: loss improved from 0.14483 to 0.10281, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-02.hdf5\n",
      "Epoch 3/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0917 - acc: 0.9693\n",
      "\n",
      "Epoch 00003: loss improved from 0.10281 to 0.09175, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-03.hdf5\n",
      "Epoch 4/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0850 - acc: 0.9714\n",
      "\n",
      "Epoch 00004: loss improved from 0.09175 to 0.08504, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-04.hdf5\n",
      "Epoch 5/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0809 - acc: 0.9730\n",
      "\n",
      "Epoch 00005: loss improved from 0.08504 to 0.08091, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-05.hdf5\n",
      "Epoch 6/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0771 - acc: 0.9743\n",
      "\n",
      "Epoch 00006: loss improved from 0.08091 to 0.07711, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-06.hdf5\n",
      "Epoch 7/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0745 - acc: 0.9750\n",
      "\n",
      "Epoch 00007: loss improved from 0.07711 to 0.07446, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-07.hdf5\n",
      "Epoch 8/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0713 - acc: 0.9764\n",
      "\n",
      "Epoch 00008: loss improved from 0.07446 to 0.07126, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-08.hdf5\n",
      "Epoch 9/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0696 - acc: 0.9767\n",
      "\n",
      "Epoch 00009: loss improved from 0.07126 to 0.06963, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-09.hdf5\n",
      "Epoch 10/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0674 - acc: 0.9775\n",
      "\n",
      "Epoch 00010: loss improved from 0.06963 to 0.06744, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-10.hdf5\n",
      "Epoch 11/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0654 - acc: 0.9784\n",
      "\n",
      "Epoch 00011: loss improved from 0.06744 to 0.06544, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-11.hdf5\n",
      "Epoch 12/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0646 - acc: 0.9786\n",
      "\n",
      "Epoch 00012: loss improved from 0.06544 to 0.06463, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-12.hdf5\n",
      "Epoch 13/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0620 - acc: 0.9794\n",
      "\n",
      "Epoch 00013: loss improved from 0.06463 to 0.06201, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-13.hdf5\n",
      "Epoch 14/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0614 - acc: 0.9798\n",
      "\n",
      "Epoch 00014: loss improved from 0.06201 to 0.06143, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-14.hdf5\n",
      "Epoch 15/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0597 - acc: 0.9801\n",
      "\n",
      "Epoch 00015: loss improved from 0.06143 to 0.05967, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-15.hdf5\n",
      "Epoch 16/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0597 - acc: 0.9803\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.05967\n",
      "Epoch 17/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0586 - acc: 0.9807\n",
      "\n",
      "Epoch 00017: loss improved from 0.05967 to 0.05860, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-17.hdf5\n",
      "Epoch 18/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0584 - acc: 0.9806\n",
      "\n",
      "Epoch 00018: loss improved from 0.05860 to 0.05844, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-18.hdf5\n",
      "Epoch 19/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0567 - acc: 0.9810\n",
      "\n",
      "Epoch 00019: loss improved from 0.05844 to 0.05673, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-19.hdf5\n",
      "Epoch 20/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0567 - acc: 0.9810\n",
      "\n",
      "Epoch 00020: loss improved from 0.05673 to 0.05673, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-20.hdf5\n",
      "Epoch 21/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0561 - acc: 0.9816\n",
      "\n",
      "Epoch 00021: loss improved from 0.05673 to 0.05610, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-21.hdf5\n",
      "Epoch 22/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0550 - acc: 0.9818\n",
      "\n",
      "Epoch 00022: loss improved from 0.05610 to 0.05501, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-22.hdf5\n",
      "Epoch 23/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0550 - acc: 0.9817\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.05501\n",
      "Epoch 24/100\n",
      "431987/431987 [==============================] - 9s 20us/step - loss: 0.0539 - acc: 0.9820\n",
      "\n",
      "Epoch 00024: loss improved from 0.05501 to 0.05394, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-24.hdf5\n",
      "Epoch 25/100\n",
      "431987/431987 [==============================] - 9s 20us/step - loss: 0.0537 - acc: 0.9822\n",
      "\n",
      "Epoch 00025: loss improved from 0.05394 to 0.05366, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-25.hdf5\n",
      "Epoch 26/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0531 - acc: 0.9823\n",
      "\n",
      "Epoch 00026: loss improved from 0.05366 to 0.05314, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-26.hdf5\n",
      "Epoch 27/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0521 - acc: 0.9826\n",
      "\n",
      "Epoch 00027: loss improved from 0.05314 to 0.05215, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-27.hdf5\n",
      "Epoch 28/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0521 - acc: 0.9827\n",
      "\n",
      "Epoch 00028: loss improved from 0.05215 to 0.05210, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-28.hdf5\n",
      "Epoch 29/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0508 - acc: 0.9829\n",
      "\n",
      "Epoch 00029: loss improved from 0.05210 to 0.05080, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-29.hdf5\n",
      "Epoch 30/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0508 - acc: 0.9832\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.05080\n",
      "Epoch 31/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0508 - acc: 0.9831\n",
      "\n",
      "Epoch 00031: loss improved from 0.05080 to 0.05079, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-31.hdf5\n",
      "Epoch 32/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0499 - acc: 0.9832\n",
      "\n",
      "Epoch 00032: loss improved from 0.05079 to 0.04988, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-32.hdf5\n",
      "Epoch 33/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0503 - acc: 0.9833\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.04988\n",
      "Epoch 34/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0500 - acc: 0.9835\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.04988\n",
      "Epoch 35/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0494 - acc: 0.9838\n",
      "\n",
      "Epoch 00035: loss improved from 0.04988 to 0.04939, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-35.hdf5\n",
      "Epoch 36/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0492 - acc: 0.9836\n",
      "\n",
      "Epoch 00036: loss improved from 0.04939 to 0.04916, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-36.hdf5\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0492 - acc: 0.9837\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.04916\n",
      "Epoch 38/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0480 - acc: 0.9840\n",
      "\n",
      "Epoch 00038: loss improved from 0.04916 to 0.04800, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-38.hdf5\n",
      "Epoch 39/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0482 - acc: 0.9840\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.04800\n",
      "Epoch 40/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0480 - acc: 0.9842\n",
      "\n",
      "Epoch 00040: loss improved from 0.04800 to 0.04796, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-40.hdf5\n",
      "Epoch 41/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0477 - acc: 0.9843\n",
      "\n",
      "Epoch 00041: loss improved from 0.04796 to 0.04766, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-41.hdf5\n",
      "Epoch 42/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0479 - acc: 0.9842\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.04766\n",
      "Epoch 43/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0467 - acc: 0.9843\n",
      "\n",
      "Epoch 00043: loss improved from 0.04766 to 0.04673, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-43.hdf5\n",
      "Epoch 44/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0472 - acc: 0.9841\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.04673\n",
      "Epoch 45/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0467 - acc: 0.9844\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.04673\n",
      "Epoch 46/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0460 - acc: 0.9848\n",
      "\n",
      "Epoch 00046: loss improved from 0.04673 to 0.04598, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-46.hdf5\n",
      "Epoch 47/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0465 - acc: 0.9844\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.04598\n",
      "Epoch 48/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0458 - acc: 0.9851\n",
      "\n",
      "Epoch 00048: loss improved from 0.04598 to 0.04577, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-48.hdf5\n",
      "Epoch 49/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0456 - acc: 0.9850\n",
      "\n",
      "Epoch 00049: loss improved from 0.04577 to 0.04562, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-49.hdf5\n",
      "Epoch 50/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0455 - acc: 0.9849\n",
      "\n",
      "Epoch 00050: loss improved from 0.04562 to 0.04547, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-50.hdf5\n",
      "Epoch 51/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0455 - acc: 0.9849\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.04547\n",
      "Epoch 52/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0457 - acc: 0.9848\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.04547\n",
      "Epoch 53/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0456 - acc: 0.9849\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.04547\n",
      "Epoch 54/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0453 - acc: 0.9848\n",
      "\n",
      "Epoch 00054: loss improved from 0.04547 to 0.04532, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-54.hdf5\n",
      "Epoch 55/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0449 - acc: 0.9849\n",
      "\n",
      "Epoch 00055: loss improved from 0.04532 to 0.04490, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-55.hdf5\n",
      "Epoch 56/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0453 - acc: 0.9851\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.04490\n",
      "Epoch 57/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0443 - acc: 0.9853\n",
      "\n",
      "Epoch 00057: loss improved from 0.04490 to 0.04434, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-57.hdf5\n",
      "Epoch 58/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0455 - acc: 0.9850\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.04434\n",
      "Epoch 59/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0441 - acc: 0.9852\n",
      "\n",
      "Epoch 00059: loss improved from 0.04434 to 0.04405, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-59.hdf5\n",
      "Epoch 60/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0443 - acc: 0.9853\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.04405\n",
      "Epoch 61/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0441 - acc: 0.9853\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.04405\n",
      "Epoch 62/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0433 - acc: 0.9856\n",
      "\n",
      "Epoch 00062: loss improved from 0.04405 to 0.04333, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-62.hdf5\n",
      "Epoch 63/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0436 - acc: 0.9856\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.04333\n",
      "Epoch 64/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0434 - acc: 0.9855\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.04333\n",
      "Epoch 65/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0431 - acc: 0.9855\n",
      "\n",
      "Epoch 00065: loss improved from 0.04333 to 0.04314, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-65.hdf5\n",
      "Epoch 66/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0431 - acc: 0.9856\n",
      "\n",
      "Epoch 00066: loss improved from 0.04314 to 0.04313, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-66.hdf5\n",
      "Epoch 67/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0429 - acc: 0.9857\n",
      "\n",
      "Epoch 00067: loss improved from 0.04313 to 0.04294, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-67.hdf5\n",
      "Epoch 68/100\n",
      "431987/431987 [==============================] - 9s 20us/step - loss: 0.0427 - acc: 0.9856\n",
      "\n",
      "Epoch 00068: loss improved from 0.04294 to 0.04273, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-68.hdf5\n",
      "Epoch 69/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0434 - acc: 0.9856\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.04273\n",
      "Epoch 70/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0424 - acc: 0.9861\n",
      "\n",
      "Epoch 00070: loss improved from 0.04273 to 0.04243, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-70.hdf5\n",
      "Epoch 71/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0431 - acc: 0.9860\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.04243\n",
      "Epoch 72/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0420 - acc: 0.9859\n",
      "\n",
      "Epoch 00072: loss improved from 0.04243 to 0.04203, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-72.hdf5\n",
      "Epoch 73/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0420 - acc: 0.9862\n",
      "\n",
      "Epoch 00073: loss improved from 0.04203 to 0.04200, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-73.hdf5\n",
      "Epoch 74/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0424 - acc: 0.9861\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.04200\n",
      "Epoch 75/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0425 - acc: 0.9857\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.04200\n",
      "Epoch 76/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0417 - acc: 0.9860\n",
      "\n",
      "Epoch 00076: loss improved from 0.04200 to 0.04169, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-76.hdf5\n",
      "Epoch 77/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0416 - acc: 0.9865\n",
      "\n",
      "Epoch 00077: loss improved from 0.04169 to 0.04162, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-77.hdf5\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0415 - acc: 0.9864\n",
      "\n",
      "Epoch 00078: loss improved from 0.04162 to 0.04150, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-78.hdf5\n",
      "Epoch 79/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0421 - acc: 0.9861\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.04150\n",
      "Epoch 80/100\n",
      "431987/431987 [==============================] - 10s 24us/step - loss: 0.0421 - acc: 0.9862\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.04150\n",
      "Epoch 81/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0413 - acc: 0.9862\n",
      "\n",
      "Epoch 00081: loss improved from 0.04150 to 0.04129, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-81.hdf5\n",
      "Epoch 82/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0412 - acc: 0.9859\n",
      "\n",
      "Epoch 00082: loss improved from 0.04129 to 0.04125, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-82.hdf5\n",
      "Epoch 83/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0417 - acc: 0.9861\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.04125\n",
      "Epoch 84/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0414 - acc: 0.9861\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.04125\n",
      "Epoch 85/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0409 - acc: 0.9864\n",
      "\n",
      "Epoch 00085: loss improved from 0.04125 to 0.04087, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-85.hdf5\n",
      "Epoch 86/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0411 - acc: 0.9863\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.04087\n",
      "Epoch 87/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0408 - acc: 0.9865\n",
      "\n",
      "Epoch 00087: loss improved from 0.04087 to 0.04084, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-87.hdf5\n",
      "Epoch 88/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0409 - acc: 0.9863\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.04084\n",
      "Epoch 89/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0415 - acc: 0.9864\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.04084\n",
      "Epoch 90/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0413 - acc: 0.9862\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.04084\n",
      "Epoch 91/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0405 - acc: 0.9864\n",
      "\n",
      "Epoch 00091: loss improved from 0.04084 to 0.04046, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-91.hdf5\n",
      "Epoch 92/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0408 - acc: 0.9865\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.04046\n",
      "Epoch 93/100\n",
      "431987/431987 [==============================] - 10s 23us/step - loss: 0.0402 - acc: 0.9866\n",
      "\n",
      "Epoch 00093: loss improved from 0.04046 to 0.04025, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-93.hdf5\n",
      "Epoch 94/100\n",
      "431987/431987 [==============================] - 9s 22us/step - loss: 0.0406 - acc: 0.9864\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.04025\n",
      "Epoch 95/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0402 - acc: 0.9866\n",
      "\n",
      "Epoch 00095: loss improved from 0.04025 to 0.04023, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-95.hdf5\n",
      "Epoch 96/100\n",
      "431987/431987 [==============================] - 10s 22us/step - loss: 0.0399 - acc: 0.9866\n",
      "\n",
      "Epoch 00096: loss improved from 0.04023 to 0.03990, saving model to /home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-96.hdf5\n",
      "Epoch 97/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0408 - acc: 0.9865\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.03990\n",
      "Epoch 98/100\n",
      "431987/431987 [==============================] - 9s 20us/step - loss: 0.0407 - acc: 0.9864\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.03990\n",
      "Epoch 99/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0403 - acc: 0.9868\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.03990\n",
      "Epoch 100/100\n",
      "431987/431987 [==============================] - 9s 21us/step - loss: 0.0406 - acc: 0.9864\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.03990\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"/home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='loss')\n",
    "csv_logger = CSVLogger('/home/bsplab/Desktop/manikanta/CNN_mani/logs/output1.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=100,batch_size=128,callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"/home/bsplab/Desktop/manikanta/CNN_mani/logs/cnn_mcsvfile.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.076202392578125ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "#model.save('manikanta_FeedforwardNN_4class.h5')\n",
    "cnn.save(\"/home/bsplab/Desktop/manikanta/CNN_mani/logs/cnn_mcsvfile.hdf5\")\n",
    "toc = time.time()\n",
    "print(str(1000*(toc-tic))+\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Plot a confusion matrix.\n",
    "# cm is the confusion matrix, names are the names of the classes.\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "acc\n",
      "0.991\n",
      "rec\n",
      "0.991\n",
      "pre\n",
      "0.991\n",
      "f1\n",
      "0.991\n",
      "==============================================\n",
      "[[35873     1     0     3]\n",
      " [    3 35481   178   179]\n",
      " [    3   225 35682   214]\n",
      " [    2   231   250 35671]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "cnn.load_weights(\"/home/bsplab/Desktop/manikanta/CNN_mani/logs/checkpoint-96.hdf5\")\n",
    "\n",
    "y_pred = cnn.predict_classes(X_test)\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "recall = recall_score(y_test1, y_pred , average=\"weighted\")\n",
    "precision = precision_score(y_test1, y_pred , average=\"weighted\")\n",
    "f1 = f1_score(y_test1, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"acc\")\n",
    "print(\"%.3f\" %accuracy)\n",
    "print(\"rec\")\n",
    "print(\"%.3f\" %recall)\n",
    "print(\"pre\")\n",
    "print(\"%.3f\" %precision)\n",
    "print(\"f1\")\n",
    "print(\"%.3f\" %f1)\n",
    "cm = metrics.confusion_matrix(y_test1, y_pred)\n",
    "print(\"==============================================\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test confusion matrix\n",
      "[[35873     1     0     3]\n",
      " [    3 35481   178   179]\n",
      " [    3   225 35682   214]\n",
      " [    2   231   250 35671]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cm = metrics.confusion_matrix(y_test1, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print('test confusion matrix')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, Label)\n",
    "plt.savefig('cm_cnn100ms.png',dpi=150)\n",
    "plt.savefig('cm_cnn100ms.eps',dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(5,3))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['ac&fan', 'crying','music', 'speech'],\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "plt.savefig('cnn_confusiponmatrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New data without labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x7f130c282da0>\n"
     ]
    }
   ],
   "source": [
    "loaded_mymodel= load_model('/home/bsplab/Desktop/manikanta/CNN_mani/logs/cnn_mcsvfile.hdf5')\n",
    "print(loaded_mymodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           MFCC0      MFCC1      MFCC2      MFCC3      MFCC4      MFCC5  \\\n",
      "0     -10.166330 -28.840257 -37.230772  14.160532 -10.673806   3.915022   \n",
      "1      -9.132246 -28.308959 -39.586994  19.144334 -16.303967   6.448080   \n",
      "2      -8.793318 -26.725370 -39.885100  14.762905 -21.725920   8.523211   \n",
      "3      -8.566548 -25.436487 -37.422073  14.051592 -29.393727   5.823137   \n",
      "4      -8.246559 -25.085498 -33.691972  18.270865 -35.307783   4.511325   \n",
      "5      -8.385470 -24.419451 -30.187452  18.560805 -34.559994   5.771957   \n",
      "6      -6.111275 -11.863153 -31.854287 -10.497121  -8.960644  28.290933   \n",
      "7      -2.948489 -13.609883 -49.967322   0.260375 -30.381663  47.969950   \n",
      "8       0.315184 -23.868377 -55.227601  -6.038735 -33.850086  46.549432   \n",
      "9       2.073974 -16.809247 -59.017832  -9.488300 -52.757944  25.484973   \n",
      "10      1.904706 -16.616180 -57.005626 -11.271170 -50.117561  21.366501   \n",
      "11      1.136662  -9.852493 -45.346100 -11.356098 -38.836975 -21.744510   \n",
      "12      2.210976  -6.481896 -46.732819  -6.204663 -36.142030 -17.356148   \n",
      "13      2.056426 -11.176803 -57.650404 -13.647662 -42.279577 -17.434570   \n",
      "14      0.575759  -0.742619 -50.235607  -1.130158 -26.096859  -0.769018   \n",
      "15     -2.977671  -5.851250 -54.884566 -15.069298 -30.315352   2.562873   \n",
      "16     -0.791487 -18.841841 -54.316663   4.593864 -19.747353  29.281900   \n",
      "17      1.506992 -16.753091 -65.043710  -6.850130 -43.684806  27.283451   \n",
      "18      1.427978 -14.503843 -54.781138 -10.805742 -34.297104  16.829063   \n",
      "19      2.084658 -12.451686 -61.942116   0.481737 -58.364272  -4.094154   \n",
      "20      2.749088 -10.529014 -54.998327  -0.221831 -43.055016  -1.339163   \n",
      "21      2.220110 -11.022322 -62.409571  -5.327440 -48.860456  -2.416413   \n",
      "22      0.537063   1.598170 -55.450059   9.406020 -37.355348  12.102477   \n",
      "23     -1.144384  -9.528204 -42.080355   6.184605 -19.999023  11.136937   \n",
      "24      2.184399 -19.412591 -57.783576   0.059959 -32.759929  20.649074   \n",
      "25      2.209989 -17.737599 -57.485908   1.394349 -33.518720  22.046946   \n",
      "26      1.288763  -4.949480 -41.200360   3.386462 -37.647599   2.884310   \n",
      "27      2.556197 -21.602671 -57.885732 -11.212328 -55.102237 -17.352664   \n",
      "28      2.425407  -8.149966 -46.227967   9.077265 -44.536031   5.045847   \n",
      "29      0.541400 -13.130961 -57.436444 -15.741525 -36.555591 -18.878639   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "71967 -11.152526  -7.760333  -8.314563 -10.215805  -3.858728   6.958416   \n",
      "71968 -10.718408  -8.458967 -14.161363  -7.850562  -6.157356   6.162839   \n",
      "71969 -10.340456 -11.803814 -17.753080  -2.694562  -8.250152   5.670345   \n",
      "71970 -10.306044 -11.960992 -16.711432   0.446633  -7.065397   3.893817   \n",
      "71971 -10.744648  -9.434034 -14.044674  -5.053287  -8.484110   3.172463   \n",
      "71972 -11.381956  -8.394854  -6.428749 -10.969921  -9.588070   7.460227   \n",
      "71973 -11.910249 -12.726467   2.343610  -5.296993  -3.572440  12.284984   \n",
      "71974 -11.926281 -14.383540   6.579354   2.993631  -0.499508   6.679378   \n",
      "71975 -11.948501 -14.312391   5.585586   4.385882   2.668209   5.828213   \n",
      "71976 -12.130691 -15.484768   7.960144   4.817156   5.318506   6.561164   \n",
      "71977 -12.187432 -16.352698  11.888713   5.006058   4.932879   8.002246   \n",
      "71978 -12.140863 -16.964037   8.655074   4.564840   7.404300   7.002891   \n",
      "71979 -11.992421 -16.153191   7.878162   6.199125   3.534919   2.636250   \n",
      "71980 -11.453276 -17.748628   2.278395  10.659024   4.588145   4.217260   \n",
      "71981 -11.303930 -17.964082  -0.423809   9.723692   2.473479   6.154991   \n",
      "71982 -11.374480 -13.098476   2.649001   2.884232  -4.705712   3.759877   \n",
      "71983 -11.120373 -12.037387   1.928934  -0.733022  -7.897900   3.803177   \n",
      "71984 -10.896495 -13.794937  -3.087409  -3.405969 -11.591814   4.886100   \n",
      "71985 -10.618528 -15.460027  -2.595013  -3.247140 -14.063511   8.578442   \n",
      "71986 -10.600311 -15.876263  -4.641096   0.127451  -9.119284   9.193088   \n",
      "71987 -10.959607 -14.896017  -6.467693   2.956565   0.104729   9.836171   \n",
      "71988  -9.764434   2.090000  10.787191   8.411616  -6.675949 -16.540321   \n",
      "71989  -5.123985  18.382917 -13.694395 -22.809231 -40.846327 -18.703075   \n",
      "71990  -2.072165  17.348311 -35.862618 -16.665351 -37.478678  -5.538668   \n",
      "71991   0.499194   7.092718 -55.953087 -21.956413 -53.571343 -12.901622   \n",
      "71992   1.066625  11.157816 -56.926774 -17.468879 -51.572970  -2.470647   \n",
      "71993   0.828569   3.593574 -56.091919 -26.572592 -48.184291 -15.537930   \n",
      "71994   0.519527   5.112819 -41.487739  -8.435614 -28.357780   5.273704   \n",
      "71995  -0.049751   3.699260 -37.583264 -16.542037 -28.512619  -8.455582   \n",
      "71996  -1.266581  11.164631 -30.624888  -3.891677 -30.605611   7.166921   \n",
      "\n",
      "           MFCC6      MFCC7      MFCC8      MFCC9     MFCC10     MFCC11  \\\n",
      "0      10.506053   9.558074   3.383239  -2.278555  -4.451030  -3.077933   \n",
      "1       4.509218   5.940498   0.490428   0.088990  -2.799367  -2.585552   \n",
      "2      -2.380727   5.415342  -5.052203   3.935676   2.095260   3.037759   \n",
      "3      -8.774367   3.093728  -6.019697   2.182583   2.267267   4.949162   \n",
      "4     -10.158296   2.634234  -5.470373   1.663351   0.142699   2.060347   \n",
      "5     -11.730162   2.312393  -5.815920   2.894088   1.216511   2.234271   \n",
      "6      -0.779432 -20.229812  -6.996246  10.421373   3.652338   2.172758   \n",
      "7     -21.851651   1.755839 -28.985889   7.820705   4.568049  -9.242179   \n",
      "8      -5.929225   4.493931 -18.591863 -11.353653  14.287365 -15.112918   \n",
      "9     -31.451559   0.563325 -30.081323   3.596037   4.777211   7.136150   \n",
      "10    -28.671628  -1.930173 -27.421005   2.198110   5.182956   6.202727   \n",
      "11    -34.510286  14.688388 -17.694110  42.015196  -3.537982   9.569414   \n",
      "12    -26.615639  26.203498 -16.709659  46.035562  -9.466976   1.453366   \n",
      "13    -18.898223  23.639131 -23.714049  28.240323 -26.102143 -11.012748   \n",
      "14     20.216281  16.266043  -2.675998  10.900856 -23.393256 -22.704884   \n",
      "15     16.498741  12.170371  10.355858  11.284299 -20.474709 -19.122968   \n",
      "16      6.710555 -11.122911  -5.748855   1.662139 -17.928455   4.431736   \n",
      "17    -27.804124  -2.799252 -31.746280  10.646165 -21.872754  17.863045   \n",
      "18    -19.669705 -10.476363 -19.898301   2.919642  -6.017591  10.882683   \n",
      "19    -41.300999  31.591599 -11.851841  18.585355 -26.405371  -1.940874   \n",
      "20    -13.546030  34.018243  -5.624485   3.114972 -16.493490 -16.574195   \n",
      "21     -8.413797  34.649677 -20.628549   7.234562  -9.385603 -23.334383   \n",
      "22      9.613658  32.758889 -26.246652  25.429881 -21.564354 -20.107700   \n",
      "23     19.539542   0.807520  -7.041382  10.224303 -19.502193 -10.841696   \n",
      "24     -9.228183   9.969960 -29.604262   8.313567 -10.914674  -5.116916   \n",
      "25    -10.200595  12.331509 -29.896652  11.359869 -10.625960  -2.796701   \n",
      "26     -7.205724  29.938159   7.688593  18.767207 -19.180093 -20.660635   \n",
      "27    -26.659007  23.753342  -1.867998  20.768213 -16.814976 -23.747249   \n",
      "28    -16.717682  39.974744 -10.099285  30.560179 -21.414746 -22.207738   \n",
      "29     22.538549  24.579999  -1.095338  14.974695  -0.156897 -38.753840   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "71967  -5.584905  13.809028  -9.249198   0.700496 -18.135939  12.905158   \n",
      "71968  -6.866258   8.569914 -12.135788   3.106468 -18.839664   9.912411   \n",
      "71969  -7.206867   8.478519  -9.753324   9.554359 -11.040859  16.645149   \n",
      "71970  -9.415521   5.848987  -9.754004  12.965361  -6.322280  19.428198   \n",
      "71971  -4.885362   6.385323 -12.913053   9.607566  -9.081001  17.142691   \n",
      "71972   4.582470  11.802419 -14.284907   7.104350  -9.630411  16.856963   \n",
      "71973   6.991052  17.772585  -7.627759  10.948771  -8.625442  13.171989   \n",
      "71974  -5.627926   7.367411 -11.696596   9.158109  -6.989362  12.142721   \n",
      "71975  -9.286794   5.017237 -11.235685  10.936831  -7.273022  11.336882   \n",
      "71976  -7.143717  10.322119  -6.598455  15.401873  -5.976891  12.407574   \n",
      "71977  -5.839807   9.760550  -3.146375  18.900065  -2.534134  13.095170   \n",
      "71978  -6.454061  12.185203  -3.054772  17.671684  -4.223207  11.224119   \n",
      "71979  -6.572148  17.168820  -3.605378  12.863989  -3.760281  13.350840   \n",
      "71980 -10.367357  19.729331  -5.921677   9.508885  -5.377596  15.966382   \n",
      "71981  -8.330261  17.095308  -8.326449   7.843823  -8.932638  16.296082   \n",
      "71982  -5.075268  12.298412 -11.141299   9.002900  -9.953235  21.102102   \n",
      "71983  -8.833498  11.148062 -15.099599   6.953764 -12.256944  20.734644   \n",
      "71984  -7.672232  10.746388 -13.759277   5.448365 -15.677881  18.664504   \n",
      "71985  -8.036260  13.006628 -13.717112   4.107270 -17.664972  16.026882   \n",
      "71986  -9.673698  11.049810 -16.538562   5.186785 -16.504467  13.253406   \n",
      "71987  -4.344690  13.281740  -8.847900  15.249958  -7.549437  14.893191   \n",
      "71988 -35.300261  -9.069357 -21.240698   1.524002 -24.017380  -9.582736   \n",
      "71989  -1.038974  -4.738495 -39.520084  11.495731 -21.197025   0.327692   \n",
      "71990   7.154656   0.413079 -18.195552  30.695526 -35.309272  10.669659   \n",
      "71991   0.865902 -15.590799  -5.619076  21.723355 -31.994703  17.817536   \n",
      "71992  -1.456713  -6.686328   0.830791  35.163688 -36.348879  19.056528   \n",
      "71993  -8.529746 -20.765832  -0.826631  33.419497 -39.920316  15.147870   \n",
      "71994 -12.589499 -12.165609 -34.938113  49.358983 -47.644821  40.666242   \n",
      "71995 -20.236559 -21.983267 -52.412383  54.366752 -45.369777  37.028230   \n",
      "71996 -12.666129  -8.371182 -48.650629  42.780694 -59.483799  26.040852   \n",
      "\n",
      "          MFCC12  \n",
      "0      -1.077127  \n",
      "1       2.802776  \n",
      "2       6.023214  \n",
      "3       2.867329  \n",
      "4       2.477675  \n",
      "5       3.249251  \n",
      "6       4.510331  \n",
      "7      11.136455  \n",
      "8       6.950031  \n",
      "9       3.873642  \n",
      "10      4.605664  \n",
      "11     -9.349495  \n",
      "12    -10.859351  \n",
      "13     -5.486365  \n",
      "14      3.142698  \n",
      "15      3.073439  \n",
      "16      0.725405  \n",
      "17      1.576194  \n",
      "18     14.007163  \n",
      "19    -19.207477  \n",
      "20     -5.975138  \n",
      "21     -8.238334  \n",
      "22     -0.870202  \n",
      "23      8.643618  \n",
      "24      6.312220  \n",
      "25      5.119899  \n",
      "26     -6.208209  \n",
      "27     -4.076838  \n",
      "28    -20.673549  \n",
      "29    -12.605648  \n",
      "...          ...  \n",
      "71967   0.305321  \n",
      "71968   0.501453  \n",
      "71969  -1.010394  \n",
      "71970  -2.339732  \n",
      "71971   0.605390  \n",
      "71972  -0.494847  \n",
      "71973  -7.056208  \n",
      "71974  -8.162602  \n",
      "71975  -6.796577  \n",
      "71976  -4.557418  \n",
      "71977  -8.512265  \n",
      "71978 -10.667123  \n",
      "71979  -7.668867  \n",
      "71980  -4.791285  \n",
      "71981  -4.501496  \n",
      "71982  -0.503609  \n",
      "71983   1.475794  \n",
      "71984   1.735240  \n",
      "71985  -0.796438  \n",
      "71986   1.748712  \n",
      "71987   3.676797  \n",
      "71988 -20.056378  \n",
      "71989  20.030896  \n",
      "71990  17.790412  \n",
      "71991   2.203668  \n",
      "71992   5.799484  \n",
      "71993  21.016666  \n",
      "71994  31.021817  \n",
      "71995  40.846865  \n",
      "71996  36.325207  \n",
      "\n",
      "[71997 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "traindata1 = pd.read_csv('/home/bsplab/Desktop/manikanta/CNN_mani/Testing_100ms/CRYING/crying_testing.csv')\n",
    "traindata1.columns=['MFCC0', 'MFCC1','MFCC2','MFCC3','MFCC4','MFCC5','MFCC6','MFCC7','MFCC8', 'MFCC9', 'MFCC10' ,'MFCC11', 'MFCC12']\n",
    "traindata1.head()\n",
    "print(traindata1)\n",
    "#traindata1.shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata1 = traindata1.values.reshape((71997,13, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240.3411865234375ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71997, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "my_his=loaded_mymodel.predict(traindata1)\n",
    "toc = time.time()\n",
    "print(str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "my_his.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71997,)\n"
     ]
    }
   ],
   "source": [
    "test_pred = np.argmax(my_his, axis=1)\n",
    "test_pred[150:380]\n",
    "print(test_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tested audio is Crying\n",
      "Count is 65352\n"
     ]
    }
   ],
   "source": [
    "c =0\n",
    "for i in  test_pred:\n",
    "    if i== 1:\n",
    "         c += 1\n",
    "    else:\n",
    "        continue \n",
    "if c > test_pred.size/2:\n",
    "    print(\"The tested audio is Crying\")\n",
    "else:\n",
    "    print(\"Error\")\n",
    "       \n",
    "        \n",
    "print(\"Count is \" + str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP=sum(test_pred==0)\n",
    "TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is for testing with new environment data of Crying: 90.7704487686987\n"
     ]
    }
   ],
   "source": [
    "x=c/test_pred.size\n",
    "print('Accuracy is for testing with new environment data of Crying:',100*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MFCC0      MFCC1      MFCC2      MFCC3      MFCC4      MFCC5  \\\n",
      "0     -2.136286  18.082098  23.966619  12.068622  23.338320   1.779057   \n",
      "1     -2.784024  11.827223  30.342649  24.918855  35.260922  -0.852137   \n",
      "2     -2.011540  19.497970  27.626973  18.961353  25.172313   0.991883   \n",
      "3     -1.705295  18.844853  32.961204  25.221247  23.003481  -2.676496   \n",
      "4     -2.508776  16.997048  25.543736  15.155044  23.828525   1.319310   \n",
      "5     -2.570069  15.296549  34.875984  23.323626  36.451120  -3.948590   \n",
      "6     -2.382347  17.494129  34.734297  19.386687  32.851699  -4.893888   \n",
      "7     -2.858220  18.167974  29.307810  14.637984  29.665675  -6.541302   \n",
      "8     -3.252421  14.095501  31.839132  22.395343  32.731965  -5.737017   \n",
      "9     -3.753252   8.264461  21.386039  22.333784  24.381523   1.801821   \n",
      "10    -3.034050   8.792350  25.561422  29.225604  29.949117   1.928972   \n",
      "11    -2.771499  15.087171  35.067742  24.434670  35.102429  -7.296454   \n",
      "12    -2.175525  13.734832  31.803575  21.556180  28.364590   2.412067   \n",
      "13    -2.394926   8.951809  28.026424  21.275815  32.102618   6.939791   \n",
      "14    -2.712146  18.562681  31.852753  17.863488  29.190656  -3.413107   \n",
      "15    -2.352567  19.602875  34.888578  20.528931  28.772667  -5.590290   \n",
      "16    -2.600164  17.492105  28.181161  15.683994  26.017221  -1.220690   \n",
      "17    -2.675070  13.617865  27.989714  18.998102  27.216464  -2.364296   \n",
      "18    -2.594859  13.737406  26.802683  20.953991  23.064274  -0.346235   \n",
      "19    -2.572861  11.291730  29.582293  28.780398  24.506520  -0.532169   \n",
      "20    -2.826187  12.505694  24.818736  18.901816  25.791549   1.758659   \n",
      "21    -2.214731  16.737354  31.740284  20.007376  31.700164  -3.876871   \n",
      "22    -1.978604  18.507575  29.060249  19.260012  30.993252  -1.952822   \n",
      "23    -2.658112  14.357040  28.126326  23.257118  30.180317  -2.579560   \n",
      "24    -3.509856  12.813897  27.601159  20.310753  29.984336  -3.371752   \n",
      "25    -3.735657  11.069995  29.988957  21.512888  33.179863  -4.955805   \n",
      "26    -3.513004   9.824223  26.296649  24.281950  30.137598  -1.118301   \n",
      "27    -3.217346  10.762696  27.135926  23.128412  31.641690  -2.703903   \n",
      "28    -2.856258  14.253509  30.109379  18.087916  30.266329  -0.351044   \n",
      "29    -2.694967  17.590753  32.427686  21.654606  26.115733  -8.541345   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "71967 -0.106807  33.168083  17.475133  10.421777  10.784697   3.839545   \n",
      "71968 -0.108486  33.374340  17.519699   9.186264  11.259191   2.105692   \n",
      "71969 -0.113752  29.666523  14.518481  11.410993  10.007137   5.908275   \n",
      "71970 -0.064927  30.897654  15.919301  11.793102  12.317088   6.424159   \n",
      "71971 -0.075893  30.219156  16.782147  11.444821  10.889985   6.491379   \n",
      "71972 -0.182844  31.985481  16.721060  10.785655  11.660534   5.867634   \n",
      "71973 -0.002645  12.104939   6.745016  10.241627   0.839717  -0.112043   \n",
      "71974  0.021737  12.399428   7.065447  10.551339   1.190808   0.270754   \n",
      "71975 -0.102663  31.128263  15.778790  11.671798  11.993557   5.678823   \n",
      "71976 -0.060802  33.409699  16.475056  10.927327  10.264863   2.609464   \n",
      "71977 -0.195111  33.115875  16.460739   9.574752  10.375463   2.743878   \n",
      "71978 -0.171166  34.817077  17.217337   9.694733  13.813366   3.099008   \n",
      "71979 -0.096213  29.915042  16.435285  12.198912  12.041573   6.733235   \n",
      "71980 -0.062551  26.102522   4.696152  21.513098  19.048240  -4.825933   \n",
      "71981 -0.107620  25.751784   6.158348  20.691362  17.227279  -4.259237   \n",
      "71982 -0.200195  31.505073  17.152568  10.744736  10.082189   4.069222   \n",
      "71983 -0.173762  30.278778  16.138269  10.134136   8.848742   4.249682   \n",
      "71984 -0.039458  12.038666   5.030321  11.613513   6.632737   7.674439   \n",
      "71985  0.119673  11.598016   1.173267   6.433618   1.481100   9.487153   \n",
      "71986  0.019926  14.935436   3.956155   6.249516  -0.280125  15.146486   \n",
      "71987 -0.226266  31.176979  16.934398  11.445951   9.259538   3.571049   \n",
      "71988 -0.170457  33.096796  17.370519  12.115081  10.229723   2.620952   \n",
      "71989 -0.082084  30.416764  16.734028  12.472918  10.253041   4.103162   \n",
      "71990 -0.080705  30.958351  16.872309  11.476303  10.839033   4.617390   \n",
      "71991 -0.025112  29.273807  17.257402  11.490255  11.077545   7.250011   \n",
      "71992  0.028760  32.953311  17.041284  12.021490  12.774121   4.341887   \n",
      "71993 -0.043432  30.297958  14.876276  11.182279  10.250485   3.731499   \n",
      "71994 -0.102069  31.363893  15.476704  10.639867  10.555574   2.683779   \n",
      "71995 -0.102418  30.728393  15.215692   9.729079  10.987224   5.764990   \n",
      "71996 -0.103544  31.931791  15.257434  10.068348   9.109493   4.296586   \n",
      "\n",
      "           MFCC6      MFCC7      MFCC8      MFCC9     MFCC10     MFCC11  \\\n",
      "0      22.591115   1.561394  15.026732   3.784844  12.519952   1.159434   \n",
      "1      20.077672  -2.165478   8.508679   1.087409  10.029992  -2.730947   \n",
      "2      22.276490  -2.474401   9.457312  -3.476956   9.278089  -3.011842   \n",
      "3      18.863127  -2.589221  12.816057  -1.697070  17.696303   0.041184   \n",
      "4      21.085362   2.940091  14.616885  -0.720633  16.035788  -0.024296   \n",
      "5      22.831394  -0.668697   7.049709   4.861674  19.078682  -4.445295   \n",
      "6      27.454374   1.725846   8.847668   2.097442  15.181075  -5.580101   \n",
      "7      22.672563  -2.377330   9.423587  -2.465930   7.588655  -5.775741   \n",
      "8      18.757056  -6.236860  10.470930  -4.032978   5.057774  -5.565681   \n",
      "9      16.435413   1.877296  15.173312  -5.222565  13.862086  -4.675334   \n",
      "10     16.428002   2.823925  17.128747  -4.824515  14.653878  -5.110278   \n",
      "11     23.866414  -0.741923  11.084100   2.348668  13.203068  -4.133221   \n",
      "12     27.364211   3.026402  12.504603   2.081142  15.046801   0.840690   \n",
      "13     30.525058   5.083605  16.371863   5.672751  15.581766   2.671949   \n",
      "14     22.326759  -0.850225  11.999837   3.273680   9.764873  -6.316788   \n",
      "15     20.316979  -0.934344   9.446038   2.071811  12.818126  -7.926210   \n",
      "16     20.199500   0.684585  10.406239  -1.138902  13.094076  -3.945266   \n",
      "17     22.537045   5.071251  14.787564   2.034724  15.697874  -1.788685   \n",
      "18     18.330471   4.888693  11.918138   1.834315  16.094838  -0.591710   \n",
      "19     15.524907   2.117086  15.174273   0.796120  16.952176  -1.821836   \n",
      "20     24.721569   3.948351  19.908059   0.404681  16.600660  -3.149546   \n",
      "21     25.698177   1.563546  11.332939   2.582891  14.636452  -4.656285   \n",
      "22     26.780839   0.713348  12.617796   2.947128   8.314545  -5.298133   \n",
      "23     22.407447  -2.513814  13.873233   2.843153   6.107539  -4.872874   \n",
      "24     22.328411  -5.702616   9.078152   1.375614  11.129391  -2.380157   \n",
      "25     23.483837  -0.594446   7.579957   3.128319  16.629114  -4.368783   \n",
      "26     22.749818  -1.033789  12.198391   1.457892   9.843513  -5.638352   \n",
      "27     22.377506  -3.153316  14.498131   3.294687  10.469580  -4.535376   \n",
      "28     23.477645  -2.553895  11.359451  -1.448729   7.065198  -4.639162   \n",
      "29     14.824413  -7.395507   7.494461  -1.045360  11.566264  -4.415031   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "71967   9.148164   3.986176  10.805096   6.218298   8.880789   1.867836   \n",
      "71968   9.942051   4.963474  14.591111   7.587373   9.447833   3.788854   \n",
      "71969   8.286127   5.043752   9.784536   6.267066   9.451222   6.707356   \n",
      "71970   9.491895   4.257440  10.877759   6.691814  11.288199   6.046724   \n",
      "71971  10.405260   6.978243  11.439393   6.258437   8.018401   3.701274   \n",
      "71972   9.558422   4.367283  10.001255   4.056479   8.127400   3.208782   \n",
      "71973   4.690376  18.157253  18.939232   6.335336  -5.126738   3.341369   \n",
      "71974   4.833022  17.965825  18.623834   6.207263  -5.081543   3.349577   \n",
      "71975   9.446805   6.160910  11.908148   7.393492  12.676487   7.427069   \n",
      "71976   9.540956   7.265766  14.525241   6.940897  10.997604   3.613492   \n",
      "71977   8.207791   4.887817  11.018161   5.729835   9.337007   2.969507   \n",
      "71978  11.688894   3.213888  11.045643   3.434627  10.613421   2.084318   \n",
      "71979  10.660080   6.096748   8.264556   3.887346   6.097303   2.132863   \n",
      "71980  14.624935   4.445859   7.437983  13.355415   2.263661   6.344223   \n",
      "71981  12.670422   5.682703   7.661706  12.369630   3.812409   6.140071   \n",
      "71982   9.134460   6.366242  10.716554   3.975156   8.896331   3.333983   \n",
      "71983  10.397638   6.804186  13.226547   9.839526   7.260456   2.022403   \n",
      "71984   2.499678   2.360952   4.066660  13.102535  16.151576  13.770708   \n",
      "71985   8.146802   4.963579   0.544854  13.051702  13.359528   4.993577   \n",
      "71986  16.126173   6.450723  -5.589052  16.345663  15.034227  -6.418582   \n",
      "71987   7.231118   5.491109  10.502473   4.940214   5.925392   1.892130   \n",
      "71988   8.394930   5.187272  11.836880   4.421875   6.566148   0.205048   \n",
      "71989   6.687905   5.240993  10.397155   5.743506   8.241109   5.559070   \n",
      "71990   8.420477   5.641098  11.236518   4.552042   9.404189   6.431336   \n",
      "71991  11.570317   8.499514  11.084113   4.714318   6.840127   4.816974   \n",
      "71992  10.333310   5.067477  12.857340   3.867036   8.143586   3.331405   \n",
      "71993   4.904646   3.167165   8.844730   7.106909  10.821548   8.257527   \n",
      "71994   4.616451   1.916481   9.909776   7.452323  11.231948   7.748472   \n",
      "71995   9.731031   6.676119  12.945191   8.416073   9.912037   3.844083   \n",
      "71996   9.202590   6.560221  12.849462   7.844646  10.005729   3.769975   \n",
      "\n",
      "          MFCC12  \n",
      "0       8.949620  \n",
      "1       8.017672  \n",
      "2       5.308946  \n",
      "3       2.939451  \n",
      "4       4.877626  \n",
      "5       7.781057  \n",
      "6       7.780306  \n",
      "7       7.484194  \n",
      "8       5.705185  \n",
      "9       5.032755  \n",
      "10      6.713317  \n",
      "11      6.224878  \n",
      "12      6.516944  \n",
      "13      8.717797  \n",
      "14      4.816720  \n",
      "15      5.219465  \n",
      "16      6.977602  \n",
      "17      7.992348  \n",
      "18      8.989925  \n",
      "19      8.425168  \n",
      "20      4.761300  \n",
      "21      7.386029  \n",
      "22      4.546532  \n",
      "23      1.109822  \n",
      "24      2.958429  \n",
      "25      4.003772  \n",
      "26      7.411902  \n",
      "27      8.470710  \n",
      "28      6.920354  \n",
      "29      4.672418  \n",
      "...          ...  \n",
      "71967   4.949789  \n",
      "71968   7.858554  \n",
      "71969   8.071029  \n",
      "71970   7.164908  \n",
      "71971   5.593528  \n",
      "71972   6.329655  \n",
      "71973  15.691735  \n",
      "71974  15.416363  \n",
      "71975   7.357533  \n",
      "71976   5.470210  \n",
      "71977   4.818466  \n",
      "71978   5.274207  \n",
      "71979   4.524471  \n",
      "71980   0.955115  \n",
      "71981   2.669469  \n",
      "71982   5.825105  \n",
      "71983   7.432441  \n",
      "71984   5.232131  \n",
      "71985   6.213845  \n",
      "71986   1.927523  \n",
      "71987   4.504585  \n",
      "71988   5.399675  \n",
      "71989   7.336815  \n",
      "71990   6.138771  \n",
      "71991   6.209723  \n",
      "71992   8.371875  \n",
      "71993  10.295119  \n",
      "71994  11.442454  \n",
      "71995   6.339400  \n",
      "71996   5.336033  \n",
      "\n",
      "[71997 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "traindata2 = pd.read_csv('/home/bsplab/Desktop/manikanta/CNN_mani/Testing_100ms/FAN_AC/acfan_testing.csv')\n",
    "traindata2.columns=['MFCC0', 'MFCC1','MFCC2','MFCC3','MFCC4','MFCC5','MFCC6','MFCC7','MFCC8', 'MFCC9', 'MFCC10' ,'MFCC11', 'MFCC12']\n",
    "traindata2.head()\n",
    "print(traindata2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata2 = traindata2.values.reshape((71997,13, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2181.574821472168ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71997, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "my_his=loaded_mymodel.predict(traindata2)\n",
    "toc = time.time()\n",
    "print(str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "my_his.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71997,)\n"
     ]
    }
   ],
   "source": [
    "test_pred1 = np.argmax(my_his, axis=1)\n",
    "test_pred1[150:380]\n",
    "print(test_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tested audio is fan&ac\n",
      "Count is 55605\n"
     ]
    }
   ],
   "source": [
    "c =0\n",
    "for i in  test_pred1:\n",
    "    if i== 0:\n",
    "         c += 1\n",
    "    else:\n",
    "        continue \n",
    "if c > test_pred1.size/2:\n",
    "    print(\"The tested audio is fan&ac\")\n",
    "else:\n",
    "    print(\"Error\")\n",
    "       \n",
    "        \n",
    "print(\"Count is \" + str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is for testing with new environment data of fan&ac: 77.23238468269511\n"
     ]
    }
   ],
   "source": [
    "x=c/test_pred1.size\n",
    "print('Accuracy is for testing with new environment data of fan&ac:',100*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MFCC0      MFCC1      MFCC2      MFCC3      MFCC4      MFCC5  \\\n",
      "0     -2.847183  -5.133256 -29.495073   3.365811  11.383593 -24.462942   \n",
      "1     -2.725085  -5.417088 -30.142874   3.098088   9.144453 -24.416763   \n",
      "2     -2.778715  -4.654355 -28.830765   4.670435   8.786832 -24.033239   \n",
      "3     -3.120197  -3.893042 -26.895662   3.886940   9.120508 -27.681915   \n",
      "4     -3.296787  -1.758949 -27.415814  -3.500055   2.782435 -29.642224   \n",
      "5     -2.619572  -0.484542 -35.122168  -9.848952   6.579182 -27.464498   \n",
      "6     -2.006199   0.398312 -38.787543 -10.754957   9.910351 -27.914611   \n",
      "7     -1.790863   0.664360 -40.456315  -7.478029  12.454894 -28.315719   \n",
      "8     -1.592627   0.577481 -42.793775  -9.321924   5.978363 -27.194942   \n",
      "9     -1.626863  -0.878060 -43.632081 -10.866416   0.530722 -26.341613   \n",
      "10    -1.564313  -0.796536 -41.666007  -6.572404   1.356740 -27.446241   \n",
      "11    -1.329345   1.772174 -38.369225  -4.905178  -0.092203 -32.970199   \n",
      "12    -1.456944  -0.698208 -37.239966  -4.736961  -8.478005 -31.619205   \n",
      "13    -1.815882  -0.855377 -33.203289   2.494609 -12.777457 -25.012442   \n",
      "14    -2.123905  -3.002858 -35.328996   0.328965 -11.370124 -16.514004   \n",
      "15    -2.185660   6.202106 -22.885464   6.767172  -6.542383  -7.108460   \n",
      "16    -1.192888   5.456399 -17.248836  18.195726 -17.135065   9.726874   \n",
      "17     0.012464   5.024920 -19.778749  23.125912 -24.098324  17.149994   \n",
      "18     0.145602   6.548814 -23.908777  16.333794 -28.993245  20.820575   \n",
      "19    -0.671797   6.025570 -25.171766  10.948487 -22.397807  14.886592   \n",
      "20    -1.523002   6.855293 -27.542636   7.773657  -2.243175  -3.887239   \n",
      "21     0.200909   4.350857 -32.099386  17.220974 -27.094565  21.921248   \n",
      "22     0.458595   5.935552 -30.656214  17.338056 -32.792816  23.011308   \n",
      "23     0.230613   2.978904 -19.184645  23.224581 -38.156710  13.727158   \n",
      "24     0.273073   4.695945 -21.589028  20.840512 -47.803119  17.341582   \n",
      "25     0.811697   2.492553 -26.504356  17.862934 -44.697971  21.908008   \n",
      "26     0.789688   2.734132 -29.557236  19.714719 -43.707550  16.515708   \n",
      "27     0.030178   4.814408 -25.239839  16.773179 -31.476325   5.158579   \n",
      "28     0.587847   6.562107 -34.121762  15.754930 -35.520304   5.717243   \n",
      "29     0.848747   5.145474 -36.992535  13.696276 -34.084613   3.669277   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "71967 -1.803252  22.967912 -32.797271  40.262911   4.676069  13.937541   \n",
      "71968 -1.906087  26.245199 -38.016378  45.020882   0.915432  13.187422   \n",
      "71969 -1.891011  25.337256 -38.992911  46.430168  -4.128723  17.354800   \n",
      "71970 -1.518205  24.835637 -41.014006  45.316613  -6.712895  15.140621   \n",
      "71971 -1.439447  22.025977 -37.424520  40.833462  -5.377491  13.833385   \n",
      "71972 -1.623221  24.338519 -41.880901  44.552922  -7.172924  15.042619   \n",
      "71973 -1.563129  19.114304 -37.863094  39.152807   0.523900  11.653875   \n",
      "71974 -1.084231  23.634801 -50.251410  44.514726  -2.461631  12.326652   \n",
      "71975 -0.705553  23.077967 -47.989064  41.921910  -2.398620  15.687144   \n",
      "71976 -1.000281  25.148520 -45.280500  39.393671  -6.313877  17.981400   \n",
      "71977 -1.700255  29.528170 -40.466107  30.060386  -5.018977  11.915059   \n",
      "71978 -0.492574  33.927259 -43.119820  17.661443 -18.954321   0.517984   \n",
      "71979 -0.465444  38.260696 -45.897945  22.079349 -21.405570  -2.260188   \n",
      "71980 -1.774434  31.657215 -30.828834  30.580536   4.590174   2.063761   \n",
      "71981 -1.629415  31.105219 -31.624065  45.988108  -6.281504   2.411841   \n",
      "71982 -1.364073  28.184101 -28.036858  41.709496 -11.068431   2.031482   \n",
      "71983 -1.597595  32.073495 -31.954568  40.160373 -10.946869   9.332605   \n",
      "71984 -1.678076  28.206786 -31.070660  30.821281  -2.165801  12.430667   \n",
      "71985 -0.563794  23.604311 -44.928638  58.062977 -20.074903  11.600611   \n",
      "71986 -0.615438  17.595178 -42.087320  62.793418 -14.873581   9.044959   \n",
      "71987 -2.048039  16.316004 -30.458827  58.925059  -6.824088   8.126586   \n",
      "71988 -2.400912  26.931105 -34.365351  51.732764 -11.079166  13.723821   \n",
      "71989 -2.265213  18.294963 -22.649322  35.207363   4.193485   9.356405   \n",
      "71990 -1.287779  29.841390 -47.299892  50.685435 -11.222094  13.915260   \n",
      "71991 -1.121728  23.829767 -33.550551  28.322480 -12.498760   6.437760   \n",
      "71992 -1.627133  36.185975 -39.873927  29.409120 -19.474559   4.248885   \n",
      "71993 -2.028610  34.703651 -38.790690  33.314506 -13.450437   6.572176   \n",
      "71994 -2.766974  34.454190 -29.228317  39.682790   1.864356   1.127293   \n",
      "71995 -2.861961  26.658249 -19.708121  34.894639  -0.825312   7.342775   \n",
      "71996 -2.374714  25.985378 -25.464917  42.958571  -3.433787  14.631594   \n",
      "\n",
      "           MFCC6      MFCC7      MFCC8      MFCC9     MFCC10     MFCC11  \\\n",
      "0      -7.501159  18.570771  -9.719536 -20.619335  10.020739  11.244722   \n",
      "1     -10.574987  17.841530 -12.135820 -22.606384   9.226490   9.056219   \n",
      "2      -9.566035  16.852608 -13.583981 -23.251187   6.774371   8.430087   \n",
      "3      -8.415228  15.028946 -14.870568 -23.106607   5.435807   9.510158   \n",
      "4     -13.836165  14.892353  -5.294564 -10.851286  11.797707   4.954623   \n",
      "5     -24.356456  16.196711  12.026252  -0.412461  10.920290   7.166371   \n",
      "6     -25.956315  16.918050  12.766473  -2.629001   7.943284  10.783702   \n",
      "7     -20.862030  15.264824   4.493280  -5.793099   5.331956  13.188335   \n",
      "8     -17.208284  16.051136   1.861985  -7.767113  11.007778  17.205454   \n",
      "9     -12.884523  19.677577   3.964771  -5.245500  12.190662  16.109013   \n",
      "10     -7.146210  25.054077  -1.539671  -7.446451  13.806233  16.427038   \n",
      "11     -7.741157  29.657298  -0.556532  -8.005250  16.710672  13.853485   \n",
      "12    -12.555497  32.972126  13.929766  -1.159703  19.892170  10.205136   \n",
      "13    -11.113822  25.145068  16.674953   2.344838  12.018877   7.076859   \n",
      "14     -7.764677  19.148273   8.208418  -4.877399   8.714301   8.283870   \n",
      "15    -14.072720  14.974067   5.449855 -18.218533   7.885427   4.762871   \n",
      "16    -37.098027   3.214302  15.156641 -35.267146   9.696171 -11.104170   \n",
      "17    -53.965125 -10.515609  20.743331 -29.867123  15.588362 -11.058513   \n",
      "18    -44.011822  -3.869608  20.808657 -36.910797  12.577180 -12.582209   \n",
      "19    -31.811357  12.218740  15.217043 -38.409170  11.803246 -13.180717   \n",
      "20    -16.773925  25.047423   3.597119 -34.010961  11.679134 -11.633533   \n",
      "21    -24.488132 -13.488531   2.631551 -50.032645   7.239821   1.131943   \n",
      "22    -28.350534 -11.270668   8.400054 -47.766126   5.968680  -4.768474   \n",
      "23    -31.810855  -5.604863  14.592063 -37.022935   7.228813  -9.552634   \n",
      "24    -19.793531  -8.205769   0.973609 -47.482524   8.882198  -6.219769   \n",
      "25    -29.070682 -11.944065  -9.128757 -50.096427   6.985047  -9.382056   \n",
      "26    -33.326254  -8.278912  -4.508622 -53.106443  10.151307 -10.220854   \n",
      "27    -33.087249   6.520174   5.308187 -37.905864   9.960974  -6.212309   \n",
      "28    -28.866876  -1.066807   8.530636 -29.022131  13.391492   1.090999   \n",
      "29    -23.374777   4.410175   8.432988 -36.685266  17.018667   4.497074   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "71967  18.924210 -20.885311  18.859203  -5.444835   2.959587  11.747004   \n",
      "71968  21.799617 -22.542736  23.729395  -5.675428  -0.856268   7.647817   \n",
      "71969  23.241367 -22.618596  26.990212  -5.515085  -2.691750   6.717142   \n",
      "71970  26.561295 -31.745570  29.203865   1.598200   2.637126   9.638643   \n",
      "71971  25.217791 -30.659991  23.879304   8.272440   3.847847  10.306179   \n",
      "71972  25.428784 -30.045916  25.967817   1.178500   4.483315  10.850136   \n",
      "71973  21.680497 -25.199223  20.776434   0.740134   0.199353  11.554234   \n",
      "71974  21.909190 -25.059440  15.255726  -1.780626  -0.507749   9.831991   \n",
      "71975  29.267049 -22.304410  21.702765  -3.753426  -2.902281   7.350889   \n",
      "71976  28.313108 -19.782320  27.115125  -3.666868  -5.488227   4.828650   \n",
      "71977  22.389376 -13.874328  23.599566  -6.981072 -10.143465  -0.540269   \n",
      "71978  25.507025 -11.518882  25.572379 -12.104890  -2.588005  11.780689   \n",
      "71979  25.185118 -18.673133  22.813196 -14.667372  -1.691825  14.281575   \n",
      "71980  14.097130 -29.019999  -4.931804 -10.275052  -7.729950   2.889703   \n",
      "71981   3.897256 -45.539977   8.327813 -12.917730  -3.049348  11.525635   \n",
      "71982   4.081031 -45.469183   9.345977 -12.234489  -3.973166  19.766016   \n",
      "71983   5.592708 -48.654624  16.185858 -11.645277  -5.707245  15.266478   \n",
      "71984   8.680623 -40.381061  16.410646  -4.729414  -6.497126  13.328244   \n",
      "71985  14.106087 -36.696604  17.920899 -11.528863   4.061945   1.714895   \n",
      "71986  17.648668 -31.272960  23.929798 -17.643284  -4.154935  -3.238580   \n",
      "71987   5.482996 -35.897589  10.857132 -23.133593  -5.658349  11.166503   \n",
      "71988   5.009452 -33.042370  14.419334 -23.722969  -9.128801   7.170300   \n",
      "71989  -0.119045 -30.899076  15.619577 -14.769449 -15.862061  -1.389944   \n",
      "71990   8.693052 -49.828977  31.149521 -15.381352  -4.652179  10.851228   \n",
      "71991   9.313208 -35.073744  25.725160  -3.305699  -2.931753  22.300571   \n",
      "71992  17.768494 -28.232494  30.143110 -13.935962   1.651735  18.600588   \n",
      "71993  23.240176 -22.849712  25.439872 -13.124762  -4.387540  14.309198   \n",
      "71994  17.213358 -35.040346  26.699467  -5.213449  -5.424928  16.706950   \n",
      "71995   8.617393 -29.589712  13.893994  -5.119155  -0.094755  12.079105   \n",
      "71996   9.597051 -35.055642  20.960835  -9.710355   4.177952   8.777946   \n",
      "\n",
      "          MFCC12  \n",
      "0     -11.562480  \n",
      "1     -13.429315  \n",
      "2     -15.121056  \n",
      "3     -12.375570  \n",
      "4     -21.112449  \n",
      "5     -21.498739  \n",
      "6     -18.202646  \n",
      "7      -4.319075  \n",
      "8       2.405649  \n",
      "9      -2.118688  \n",
      "10     -4.304131  \n",
      "11    -12.062308  \n",
      "12    -15.583619  \n",
      "13    -10.420678  \n",
      "14     -4.295284  \n",
      "15     -8.119227  \n",
      "16    -10.260201  \n",
      "17     -2.932416  \n",
      "18      2.475432  \n",
      "19     -1.503963  \n",
      "20    -10.431013  \n",
      "21      6.191941  \n",
      "22      1.904309  \n",
      "23    -12.078130  \n",
      "24     -5.743058  \n",
      "25      6.017041  \n",
      "26      1.543737  \n",
      "27     -3.739275  \n",
      "28      1.630462  \n",
      "29      3.080280  \n",
      "...          ...  \n",
      "71967 -16.545067  \n",
      "71968 -19.526796  \n",
      "71969 -17.202007  \n",
      "71970 -19.875272  \n",
      "71971 -21.294295  \n",
      "71972 -24.008537  \n",
      "71973 -20.536390  \n",
      "71974 -17.379327  \n",
      "71975 -17.355405  \n",
      "71976 -16.568431  \n",
      "71977 -15.651304  \n",
      "71978 -25.181508  \n",
      "71979 -24.830106  \n",
      "71980  -7.296341  \n",
      "71981 -10.339859  \n",
      "71982  -5.664184  \n",
      "71983 -11.526865  \n",
      "71984 -26.156253  \n",
      "71985 -18.649303  \n",
      "71986 -17.755667  \n",
      "71987 -16.744751  \n",
      "71988 -20.769288  \n",
      "71989 -19.950276  \n",
      "71990 -24.630087  \n",
      "71991 -10.336735  \n",
      "71992  -7.112674  \n",
      "71993  -8.277260  \n",
      "71994 -13.506220  \n",
      "71995 -13.602021  \n",
      "71996 -23.708263  \n",
      "\n",
      "[71997 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "traindata3 = pd.read_csv('/home/bsplab/Desktop/manikanta/CNN_mani/Testing_100ms/MUSIC/music_testing.csv')\n",
    "traindata3.columns=['MFCC0', 'MFCC1','MFCC2','MFCC3','MFCC4','MFCC5','MFCC6','MFCC7','MFCC8', 'MFCC9', 'MFCC10' ,'MFCC11', 'MFCC12']\n",
    "traindata3.head()\n",
    "print(traindata3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata3 = traindata3.values.reshape((71997,13, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2050.652027130127ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71997, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "my_his=loaded_mymodel.predict(traindata3)\n",
    "toc = time.time()\n",
    "print(str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "my_his.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71997,)\n"
     ]
    }
   ],
   "source": [
    "test_pred2 = np.argmax(my_his, axis=1)\n",
    "test_pred2[150:380]\n",
    "print(test_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tested audio is music\n",
      "Count is 66321\n"
     ]
    }
   ],
   "source": [
    "c =0\n",
    "for i in  test_pred2:\n",
    "    if i== 2:\n",
    "         c += 1\n",
    "    else:\n",
    "        continue \n",
    "if c > test_pred1.size/2:\n",
    "    print(\"The tested audio is music\")\n",
    "else:\n",
    "    print(\"Error\")\n",
    "       \n",
    "        \n",
    "print(\"Count is \" + str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is for testing with new environment data of music: 92.11633818075752\n"
     ]
    }
   ],
   "source": [
    "x=c/test_pred2.size\n",
    "print('Accuracy is for testing with new environment data of music:',100*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MFCC0      MFCC1      MFCC2      MFCC3      MFCC4      MFCC5  \\\n",
      "0     -1.913532  31.852519 -42.577216 -18.321822  -8.708340 -12.481428   \n",
      "1     -5.118569  33.345421 -30.475053   2.577803   5.102440 -21.529671   \n",
      "2     -7.092314  25.187206 -19.243648   3.264571  18.508959  -0.360402   \n",
      "3     -4.847582  32.435658  -7.349611  12.984855  19.569092  -6.030120   \n",
      "4     -4.120471  35.838699  -7.079418   6.508375  12.518670 -16.737157   \n",
      "5     -3.859222  42.890352 -18.126388  -1.586541   2.517144 -37.993624   \n",
      "6     -3.464434  36.924635 -16.566608  -8.345801  -6.749671 -38.803027   \n",
      "7     -3.547884  47.197588 -21.393149  -9.016009   2.898986 -47.028028   \n",
      "8     -4.155614  32.140166 -15.141683   3.418134  10.387392 -21.771469   \n",
      "9     -3.984728  37.820340 -30.483679  15.234969  32.531250 -27.073074   \n",
      "10    -3.663392  31.851781 -28.837347  15.175058  30.364914 -18.518461   \n",
      "11    -3.906725  34.374503 -35.363530  14.892589  38.573227 -31.246310   \n",
      "12    -4.726873  15.959966 -21.162888  41.668807  20.859056 -18.845144   \n",
      "13    -4.584198  -1.083652 -39.841585  61.845985  18.951707 -37.109775   \n",
      "14    -4.234825  -4.894774 -42.816288  39.972137   9.838501 -24.655003   \n",
      "15    -4.495435  10.076329 -53.128904  39.676086  29.906257 -34.075300   \n",
      "16    -5.153808   2.382466 -41.499309  33.165297  27.261238 -15.678979   \n",
      "17    -4.305037  30.080875 -27.450496  32.560433  46.493890 -32.914378   \n",
      "18    -3.799413  30.843153 -25.229274  28.414150  40.862492 -28.458588   \n",
      "19    -4.236457  29.841900 -22.168142  32.768686  33.241019 -22.474370   \n",
      "20    -4.472142  13.219097 -11.498155  54.119863  -8.805603 -25.749969   \n",
      "21    -4.234307   4.284291 -25.581779  69.544926   3.184966 -26.162549   \n",
      "22    -3.776598  18.828147 -20.359369  37.018689  -6.311050 -17.822099   \n",
      "23    -2.320471  26.792330 -34.456909   8.994177  12.572635 -19.189396   \n",
      "24    -2.444625  32.534818 -36.727078   5.841904  18.202224 -14.437938   \n",
      "25    -5.581993  15.908749 -13.761475   4.512841  21.295637  -0.477289   \n",
      "26    -7.420628  21.313409 -23.718154  -0.948320  36.172962   6.469809   \n",
      "27    -4.012092  39.686024 -19.741907  -9.411040  29.529327   6.068695   \n",
      "28    -3.955111  41.801115 -19.180273 -11.349777  36.385370   8.140567   \n",
      "29    -6.410690  31.674737  -4.779899  -1.624429  20.218476   0.322549   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "71967 -5.230604  32.762023 -25.058669 -10.887103 -11.598695 -34.608806   \n",
      "71968 -7.578185  25.397304 -17.298573  -1.981959  -3.556066 -31.703087   \n",
      "71969 -5.208396  35.150038 -26.632732  -2.328781  -2.512736 -34.026893   \n",
      "71970 -5.126301  37.029392 -28.117735  -3.950962   2.205134 -31.858788   \n",
      "71971 -6.433432  31.371514 -19.953622  -3.380544  16.955884 -15.530057   \n",
      "71972 -7.033948  28.617345 -17.498496  -1.671329  20.844729 -12.437766   \n",
      "71973 -7.033752  29.050628 -16.320275   3.240274   6.470849 -30.376309   \n",
      "71974 -5.526786  34.517208 -20.574027  -3.538338  -6.981505 -40.750844   \n",
      "71975 -5.094757  31.496303 -15.692555  -8.509664 -16.162176 -28.741219   \n",
      "71976 -5.176739  35.927343 -19.128426 -11.965545 -14.508684 -29.147572   \n",
      "71977 -5.506112  30.593373 -14.793079 -15.142596 -15.781777 -14.925306   \n",
      "71978 -6.671657  28.885128 -18.301561  -9.118668  -8.567248 -19.062764   \n",
      "71979 -6.215864   4.897460 -32.975954  42.178091   1.445625 -34.608542   \n",
      "71980 -5.517076  -6.271113 -49.025788  35.705752  -1.618526 -30.324113   \n",
      "71981 -5.337795  20.855430 -36.053864  14.939850 -13.932119 -22.504912   \n",
      "71982 -4.317418  28.352577 -32.137544  -5.527804  -7.119534 -15.474085   \n",
      "71983 -4.091666  32.246961 -35.200405  -6.505988  -5.723829 -16.405200   \n",
      "71984 -5.050005  28.847071 -24.907974 -11.322506   4.021189  -5.215962   \n",
      "71985 -7.855164  24.705494 -16.026500   2.280802  24.916909  -5.534520   \n",
      "71986 -8.326902  23.113272 -14.773598   2.929935  22.515316  -7.201129   \n",
      "71987 -6.847319  25.035039 -25.664665  13.018343  33.054652 -14.689952   \n",
      "71988 -5.420382  26.886948 -36.736478  16.610199  46.073678 -12.450028   \n",
      "71989 -5.595871  23.512920 -37.637385  21.444208  48.852487 -12.284352   \n",
      "71990 -7.691574   2.261467 -31.496807  38.846780  25.409336 -20.250723   \n",
      "71991 -5.454401  -8.496067 -62.118685  36.955107  27.911403 -28.032841   \n",
      "71992 -5.210866  16.191100 -46.397065  35.666799  28.736621 -16.229495   \n",
      "71993 -5.946608  24.495271 -32.512473  13.609874  27.338121  -4.421281   \n",
      "71994 -6.244325  23.657233 -31.639875  16.078842  22.545561 -12.290788   \n",
      "71995 -5.913368  12.592518 -33.872228  36.035516   7.297822 -17.322707   \n",
      "71996 -5.703516 -11.145279 -49.655642  35.258103   1.942547 -17.097016   \n",
      "\n",
      "           MFCC6      MFCC7      MFCC8      MFCC9     MFCC10     MFCC11  \\\n",
      "0       0.241599 -29.995992 -16.237165  -3.328247 -12.419673   4.719616   \n",
      "1      -5.802911 -28.950510 -16.973964 -14.963534  -3.485879  15.786829   \n",
      "2      27.645750 -14.871586 -37.902458   0.441607   2.883835   1.437866   \n",
      "3       6.437001 -37.261577 -54.996344 -18.002442  -7.837272  -4.023736   \n",
      "4      -7.681903 -15.656029 -29.380684 -28.421864 -24.852819 -10.749553   \n",
      "5      -9.693086   0.735366  -4.079341 -14.241087 -30.110862 -13.278090   \n",
      "6     -20.476145   8.513942   5.231886 -19.720686 -30.728392 -10.109757   \n",
      "7     -10.459661   5.355313  -4.739444 -21.959153 -35.402983   0.540099   \n",
      "8     -11.954585 -13.332146 -34.953920 -37.636172 -25.510092  -1.686450   \n",
      "9     -12.561243 -42.961712 -34.056331  -6.683402 -28.664216  -3.452802   \n",
      "10    -28.004139 -39.656097 -22.583585  -6.528678 -21.852747  -7.855735   \n",
      "11    -23.741513 -41.782434 -28.802062  -4.506677 -27.976178 -14.667943   \n",
      "12     -4.407502 -44.044766 -36.946827   2.392969 -16.872980 -15.345061   \n",
      "13     39.128519 -36.140501 -36.434371   7.092822 -22.397802  -1.065309   \n",
      "14     18.451598 -16.764884 -25.931372   6.350503  -8.926950  -7.564592   \n",
      "15     30.716082 -20.679255 -24.213669  19.298597  -5.932955   3.468050   \n",
      "16      6.558830  -6.483181 -16.449082  17.174026  11.237754  -7.890091   \n",
      "17    -28.140421 -45.893582 -37.352678  -4.049899 -20.422896 -14.342053   \n",
      "18    -32.233434 -50.940037 -35.635562  -5.905710 -25.121071 -16.184278   \n",
      "19    -20.126581 -61.173791 -34.827144   0.611464 -30.126737 -12.136422   \n",
      "20     15.466187 -56.902286 -43.000140  -1.057698 -22.104171  -4.610583   \n",
      "21     43.857707 -49.338006 -25.660582  11.206698 -23.362142  12.757431   \n",
      "22     14.645654 -58.678376 -42.459072   9.200403  -3.635928  12.959027   \n",
      "23    -22.078667 -55.396853 -28.975080  11.863602   0.892790   3.304267   \n",
      "24    -26.152809 -50.338419 -17.880303   8.497773   5.260628  12.954736   \n",
      "25    -28.998126 -37.674616 -15.599148   7.084755   8.608453  -5.115144   \n",
      "26      3.220547 -30.227518 -28.158499   7.530678  -4.856936  -7.677753   \n",
      "27    -17.480242 -56.149179 -45.720467  -8.054433 -12.119612  -8.386576   \n",
      "28    -17.269592 -64.504167 -54.466747 -11.504300 -18.505181  -5.299205   \n",
      "29    -11.239669 -34.489647 -21.931453   4.040518  -9.503432 -10.721996   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "71967  27.861808  11.275810 -29.469921 -18.017674   0.167806  31.144288   \n",
      "71968  14.108927  -2.777118 -18.558552  -3.384846   1.304104  19.918707   \n",
      "71969  10.227271  -0.434684  -7.232536  -4.704450  -4.958498  24.130023   \n",
      "71970   6.649246  -3.591825  -3.006918  -0.185777  -7.077166  16.756941   \n",
      "71971  -3.600206  -3.837186   3.170770  10.334068  -1.568331 -12.954612   \n",
      "71972   0.819240  -1.445106   4.554955  13.761080  -2.993332 -20.167999   \n",
      "71973  -0.176816  -8.859654   1.810083  14.261355 -11.820416  -9.114342   \n",
      "71974   1.503734 -10.316369   3.112508  13.322258 -12.280081   8.602288   \n",
      "71975   0.130815 -11.070545  -0.230979  10.879754  -8.516046  11.860000   \n",
      "71976  11.782445 -20.658728 -11.222429  18.124033 -11.504973  10.699821   \n",
      "71977   6.674329 -20.596609 -15.167565   7.099115  -1.765471   9.662387   \n",
      "71978  21.560702 -18.884171 -29.236765  10.254828   3.409218   5.244321   \n",
      "71979  39.612709 -22.759678 -24.255767   6.791085 -11.019017  19.995053   \n",
      "71980  41.123479 -20.820295 -14.031318  15.426108  -7.756203  16.000367   \n",
      "71981  37.234509 -23.332378 -24.076600  14.190633   3.121518  19.873403   \n",
      "71982  20.954868 -16.599147 -26.559512   7.857281  13.236127  32.532069   \n",
      "71983  20.856595 -18.960067 -25.478407   5.376645  10.512002  34.690820   \n",
      "71984   7.773915 -17.637459 -25.802855  -5.428410   7.869301  30.869161   \n",
      "71985   9.428638 -15.433724 -22.364635   1.180179  -2.237602  -1.854417   \n",
      "71986  15.129062  -1.222222 -14.341543  -9.230839 -22.968298  -1.761543   \n",
      "71987  10.160370  -2.671368 -18.469158 -14.758044 -22.768996   5.175533   \n",
      "71988   2.479143 -18.911329 -22.057493  -8.360448 -20.066850   7.755189   \n",
      "71989   1.296427 -25.887277 -25.383841  -4.046416 -15.264203   8.281244   \n",
      "71990  19.061191 -12.154548  -4.774520   6.662003 -14.472201  -2.405671   \n",
      "71991  25.619555 -16.664347   4.678564  24.304521 -14.829955   6.862841   \n",
      "71992  23.523851 -39.598286 -32.240743   3.203256 -10.701811  10.942056   \n",
      "71993  15.075442 -33.475201 -36.309291  -2.398178  -7.268286  14.599316   \n",
      "71994  19.891267 -25.621741 -37.616091  -6.717209  -7.869262  17.552734   \n",
      "71995  37.981603 -34.646657 -38.230613  -1.801533 -12.445190  16.578265   \n",
      "71996  52.065856 -30.827039 -22.064642  20.390963 -12.004541   9.601142   \n",
      "\n",
      "          MFCC12  \n",
      "0     -16.123570  \n",
      "1     -20.348795  \n",
      "2       1.796190  \n",
      "3      -0.837517  \n",
      "4      -1.295333  \n",
      "5      -9.092757  \n",
      "6      -3.379785  \n",
      "7      10.346780  \n",
      "8      10.917391  \n",
      "9       5.039443  \n",
      "10     -1.664523  \n",
      "11     -7.164974  \n",
      "12     -0.228514  \n",
      "13     -8.137829  \n",
      "14      5.560781  \n",
      "15      5.760790  \n",
      "16      3.937226  \n",
      "17      4.523533  \n",
      "18      3.047567  \n",
      "19      9.839782  \n",
      "20     -1.433552  \n",
      "21     -2.480618  \n",
      "22      7.189405  \n",
      "23     -4.096729  \n",
      "24     -9.361847  \n",
      "25    -10.558043  \n",
      "26      4.933247  \n",
      "27     11.185278  \n",
      "28     19.088215  \n",
      "29      4.601292  \n",
      "...          ...  \n",
      "71967  13.869972  \n",
      "71968   3.011253  \n",
      "71969   2.193918  \n",
      "71970  -1.427939  \n",
      "71971 -28.317538  \n",
      "71972 -36.642255  \n",
      "71973  -7.425764  \n",
      "71974   7.165269  \n",
      "71975  12.866542  \n",
      "71976  21.983335  \n",
      "71977  15.460207  \n",
      "71978  14.315447  \n",
      "71979  13.219067  \n",
      "71980   5.878929  \n",
      "71981  10.242023  \n",
      "71982  14.843019  \n",
      "71983  15.675576  \n",
      "71984  13.531743  \n",
      "71985 -10.171087  \n",
      "71986   9.986625  \n",
      "71987   4.645036  \n",
      "71988   0.491126  \n",
      "71989  -1.492938  \n",
      "71990  -8.076413  \n",
      "71991  -0.145189  \n",
      "71992  -4.615482  \n",
      "71993   2.930470  \n",
      "71994   6.713430  \n",
      "71995   5.901565  \n",
      "71996   6.158725  \n",
      "\n",
      "[71997 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "traindata4 = pd.read_csv('/home/bsplab/Desktop/manikanta/CNN_mani/Testing_100ms/SPEECH/speech_testing.csv')\n",
    "traindata4.columns=['MFCC0', 'MFCC1','MFCC2','MFCC3','MFCC4','MFCC5','MFCC6','MFCC7','MFCC8', 'MFCC9', 'MFCC10' ,'MFCC11', 'MFCC12']\n",
    "traindata4.head()\n",
    "print(traindata4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata4 = traindata4.values.reshape((71997,13, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256.307363510132ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71997, 4)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "my_his=loaded_mymodel.predict(traindata4)\n",
    "toc = time.time()\n",
    "print(str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "my_his.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71997,)\n"
     ]
    }
   ],
   "source": [
    "test_pred3 = np.argmax(my_his, axis=1)\n",
    "test_pred3[150:380]\n",
    "print(test_pred3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tested audio is speech\n",
      "Count is 61302\n"
     ]
    }
   ],
   "source": [
    "c =0\n",
    "for i in  test_pred3:\n",
    "    if i== 3:\n",
    "         c += 1\n",
    "    else:\n",
    "        continue \n",
    "if c > test_pred3.size/2:\n",
    "    print(\"The tested audio is speech\")\n",
    "else:\n",
    "    print(\"Error\")\n",
    "       \n",
    "        \n",
    "print(\"Count is \" + str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is for testing with new environment data of speech: 85.14521438393267\n"
     ]
    }
   ],
   "source": [
    "x=c/test_pred3.size\n",
    "print('Accuracy is for testing with new environment data of speech:',100*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
